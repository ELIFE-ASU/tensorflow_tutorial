{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train\n",
    "test_x, test_y = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x, dtype=float)\n",
    "train_y = np.asarray(train_y, dtype=int)\n",
    "test_x = np.asarray(test_x, dtype=float)\n",
    "test_y = np.asarray(test_y, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(ny, nx)\n",
    "fig.subplots_adjust(wspace=-0.7, hspace=0.1)\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        ax[i][j].imshow(train_x[ny*i+j], cmap='gray')\n",
    "        ax[i][j].axis('off')\n",
    "\n",
    "plt.show()        \n",
    "print(np.reshape(train_y[:nx * ny], [nx, ny]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](nn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph for dense neural network.\n",
    "def build_dnn(input_size, hidden_layers, output_size):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "    y_ = tf.placeholder(tf.int64, shape=(None,))\n",
    "    prev_size = input_size\n",
    "    \n",
    "    h = x\n",
    "    for i, layer_size in enumerate(hidden_layers):\n",
    "        ### FIX ME\n",
    "    \n",
    "    with tf.name_scope(\"fc_out\"):\n",
    "        ### FIX ME\n",
    "        \n",
    "    return x, logits, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data stream.\n",
    "def data_batches(features, labels, batch_size, shuffle=False):\n",
    "    if shuffle:\n",
    "        perm = np.random.permutation(len(features))\n",
    "        features = features[perm]\n",
    "        labels = labels[perm]\n",
    "        \n",
    "    i = 0\n",
    "    while i < len(features):\n",
    "        yield features[i:i+batch_size], labels[i:i+batch_size]\n",
    "        i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x, logits, y_ = build_dnn(28*28, [2000, 2000], 10)\n",
    "\n",
    "loss = \n",
    "train_op = \n",
    "\n",
    "prediction = \n",
    "accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Session.\n",
    "sess = \n",
    "# Create a writer.\n",
    "writer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "start_time = time.time()\n",
    "for step in range(2000):\n",
    "    # if (step + 1) % 100 == 0 or step == 0:\n",
    "    #     test_accuracy = \n",
    "    #     train_loss = \n",
    "        \n",
    "        # Add scalar summary to tensorboard\n",
    "        # writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"test_accuracy\", simple_value=test_accuracy),\n",
    "        #                                      tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss)]),\n",
    "        #                    step)\n",
    "        \n",
    "        # Print basic training info.\n",
    "    #     dtime = time.time() - start_time\n",
    "    #     start_time = time.time()\n",
    "    #     print(\"Step {} ({:.3f}s), loss: {}, test accuracy: {} \\t \".format(\n",
    "    #             step+1, dtime, train_loss, test_accuracy))\n",
    "    \n",
    "    \n",
    "    train_data_batches = data_batches(train_x, train_y, batch_size=None, shuffle=True)\n",
    "    for batch_x, batch_y in train_data_batches:\n",
    "        ### FIX ME\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(ny, nx)\n",
    "fig.subplots_adjust(wspace=-0.7, hspace=0.1)\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        ax[i][j].imshow(test_x[ny*i+j], cmap='gray')\n",
    "        ax[i][j].axis('off')\n",
    "\n",
    "plt.show()   \n",
    "\n",
    "predicted = sess.run(prediction, feed_dict={x: np.reshape(test_x[:nx*ny], [-1, 28*28])})\n",
    "print(np.reshape(predicted, [nx, ny]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Convolution Animation](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_size, channels, fc_layers, output_size):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, input_size, input_size])\n",
    "    y_ = tf.placeholder(tf.int64, shape=[None])\n",
    "    \n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, input_size, input_size, 1])\n",
    "    \n",
    "    prev = x_image\n",
    "    prev_c = 1\n",
    "    for i, c in enumerate(channels):\n",
    "        with tf.name_scope('conv{}'.format(i)):\n",
    "            W_conv = tf.Variable(tf.truncated_normal([3, 3, prev_c, c]))\n",
    "            h_conv = tf.nn.relu(tf.nn.conv2d(prev, W_conv, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        \n",
    "        with tf.name_scope('pool{}'.format(i)):\n",
    "            h_pool = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            \n",
    "        prev = h_pool\n",
    "        prev_c = c\n",
    "    \n",
    "    h = tf.contrib.layers.flatten(h_pool)\n",
    "    prev_size = h.get_shape().as_list()[1]\n",
    "    for i, layer_size in enumerate(fc_layers):\n",
    "        with tf.name_scope(\"fc{}\".format(i)):\n",
    "            W = tf.Variable(tf.truncated_normal([prev_size, layer_size]), name=\"W\")\n",
    "            b = tf.Variable(tf.zeros([layer_size]), name=\"b\")\n",
    "            \n",
    "            h = tf.nn.relu(tf.matmul(h, W) + b)\n",
    "            \n",
    "        prev_size = layer_size\n",
    "        \n",
    "    with tf.name_scope(\"fc_out\"):\n",
    "        W = tf.Variable(tf.truncated_normal([prev_size, output_size]), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([output_size]), name=\"b\")\n",
    "        \n",
    "        logits = tf.matmul(h, W) + b\n",
    "        \n",
    "    return x, logits, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x, logits, y_ = build_cnn(28, [32, 64], [1000], 10)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=logits)\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"mnist_cnn_raw\")\n",
    "\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(300):\n",
    "    if (step + 1) % 100 == 0 or step == 0:\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: test_x, y_: test_y})\n",
    "        sample = np.random.choice(len(train_x), 1000, replace=False)\n",
    "        train_loss = sess.run(loss, feed_dict={x: train_x[sample], y_: train_y[sample]})\n",
    "        \n",
    "        writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"test_accuracy\", simple_value=test_accuracy),\n",
    "                                             tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss)]),\n",
    "                           step)\n",
    "        \n",
    "        dtime = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print(\"Step {} ({:.3f}s), loss: {}, test accuracy: {} \\t \".format(\n",
    "                step+1, dtime, train_loss, test_accuracy))\n",
    "        \n",
    "    train_data_batches = data_batches(train_x, train_y, batch_size=128, shuffle=True)\n",
    "    for batch_x, batch_y in train_data_batches:\n",
    "        sess.run(train_op, feed_dict={x: batch_x, y_: batch_y})\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(ny, nx)\n",
    "fig.subplots_adjust(wspace=-0.7, hspace=0.1)\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        ax[i][j].imshow(test_x[ny*i+j], cmap='gray')\n",
    "        ax[i][j].axis('off')\n",
    "\n",
    "plt.show()   \n",
    "\n",
    "predicted = sess.run(prediction, feed_dict={x: np.reshape(test_x[:nx*ny], [-1, 28*28])})\n",
    "print(np.reshape(predicted, [nx, ny]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Neural Network with `Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key='x', shape=(28, 28))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dnn_classifier = tf.estimator.DNNClassifier(\n",
    "                        feature_columns=feature_columns,\n",
    "                        hidden_units=[2000, 200],\n",
    "                        n_classes=10,\n",
    "                        model_dir=\"mnist_dnn_estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features:dict, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features:dict, labels, batch_size):\n",
    "    if labels is None:\n",
    "        return tf.data.Dataset.from_tensor_slices(features).batch(batch_size)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dnn_classifier.train(input_fn=lambda: train_input_fn({\"x\": train_x}, train_y, 64), steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model with custom `Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "                inputs=input_layer,\n",
    "                filters=32,\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=64,\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #2\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "                inputs=dense, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT  and by the `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # `sparse_softmax_cross_entropy` takes class number as label\n",
    "    # `softmax_cross_entropy` takes onehot vector as label\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "                    loss=loss,\n",
    "                    global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "                        labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "                        model_fn=cnn_model_fn, model_dir=\"mnist_convnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier.train(\n",
    "    input_fn=lambda: train_input_fn({\"x\": train_x}, train_y, batch_size=64),\n",
    "    steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = mnist_classifier.evaluate(input_fn=lambda: eval_input_fn({\"x\": test_x}, test_y, batch_size=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
